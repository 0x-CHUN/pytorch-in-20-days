{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# nn.functional 和 nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "#打印时间\n",
    "def printbar():\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "nn.functional(一般引入后改名为F)有各种功能组件的函数实现。例如：\n",
    "\n",
    "(激活函数)\n",
    "\n",
    "* F.relu\n",
    "* F.sigmoid\n",
    "* F.tanh\n",
    "* F.softmax\n",
    "\n",
    "(模型层)\n",
    "\n",
    "* F.linear\n",
    "* F.conv2d\n",
    "* F.max_pool2d\n",
    "* F.dropout2d\n",
    "* F.embedding\n",
    "\n",
    "(损失函数)\n",
    "\n",
    "* F.binary_cross_entropy\n",
    "* F.mse_loss\n",
    "* F.cross_entropy\n",
    "\n",
    "为了便于对参数进行管理，一般通过继承 nn.Module 转换成为类的实现形式，并直接封装在 nn 模块下。例如：\n",
    "\n",
    "(激活函数)\n",
    "\n",
    "* nn.ReLU\n",
    "* nn.Sigmoid\n",
    "* nn.Tanh\n",
    "* nn.Softmax\n",
    "\n",
    "(模型层)\n",
    "\n",
    "* nn.Linear\n",
    "* nn.Conv2d\n",
    "* nn.MaxPool2d\n",
    "* nn.Dropout2d\n",
    "* nn.Embedding\n",
    "\n",
    "(损失函数)\n",
    "\n",
    "* nn.BCELoss\n",
    "* nn.MSELoss\n",
    "* nn.CrossEntropyLoss\n",
    "\n",
    "实际上nn.Module除了可以管理其引用的各种参数，还可以管理其引用的子模块，功能十分强大。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 使用nn.Module来管理参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4424, -1.2033],\n",
      "        [-1.0897,  0.5154]], requires_grad=True)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "w = nn.Parameter(torch.randn(2, 2))\n",
    "print(w)\n",
    "print(w.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParameterList(\n",
      "    (0): Parameter containing: [torch.FloatTensor of size 8x1]\n",
      "    (1): Parameter containing: [torch.FloatTensor of size 8x2]\n",
      ")\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "params_list = nn.ParameterList([nn.Parameter(torch.rand(8, i)) for i in range(1, 3)])\n",
    "print(params_list)\n",
    "print(params_list[0].requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParameterDict(\n",
      "    (a): Parameter containing: [torch.FloatTensor of size 2x2]\n",
      "    (b): Parameter containing: [torch.FloatTensor of size 2]\n",
      ")\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "params_dict = nn.ParameterDict({'a' : nn.Parameter(torch.rand(2, 2)),\n",
    "                                'b' : nn.Parameter(torch.zeros(2))})\n",
    "\n",
    "print(params_dict)\n",
    "print(params_dict['a'].requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4424, -1.2033],\n",
      "        [-1.0897,  0.5154]], requires_grad=True) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[0.1191],\n",
      "        [0.1693],\n",
      "        [0.6442],\n",
      "        [0.9955],\n",
      "        [0.2033],\n",
      "        [0.4559],\n",
      "        [0.2690],\n",
      "        [0.6839]], requires_grad=True) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[0.6580, 0.0894],\n",
      "        [0.9808, 0.8071],\n",
      "        [0.9853, 0.3678],\n",
      "        [0.7155, 0.0200],\n",
      "        [0.8460, 0.1202],\n",
      "        [0.0278, 0.5716],\n",
      "        [0.0180, 0.6040],\n",
      "        [0.3342, 0.3347]], requires_grad=True) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[0.2042, 0.9376],\n",
      "        [0.3878, 0.7019]], requires_grad=True) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True) \n",
      "\n",
      "number of Parameters = 5\n"
     ]
    }
   ],
   "source": [
    "module = nn.Module()\n",
    "module.w = w\n",
    "module.params_list = params_list\n",
    "module.params_dict = params_dict\n",
    "\n",
    "num_param = 0\n",
    "for param in module.parameters():\n",
    "    print(param,\"\\n\")\n",
    "    num_param = num_param + 1\n",
    "print(\"number of Parameters =\",num_param)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weight, self.bias)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 使用nn.Module来管理子模块"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=10000, embedding_dim=3, padding_idx=1)\n",
    "        self.conv = nn.Sequential()\n",
    "        self.conv.add_module(\"conv_1\", nn.Conv1d(in_channels=3, out_channels=16, kernel_size=5))\n",
    "        self.conv.add_module(\"pool_1\", nn.MaxPool1d(kernel_size=2))\n",
    "        self.conv.add_module(\"relu_1\", nn.ReLU())\n",
    "        self.conv.add_module(\"conv_2\", nn.Conv1d(in_channels=16, out_channels=128, kernel_size=2))\n",
    "        self.conv.add_module(\"pool_2\", nn.MaxPool1d(kernel_size=2))\n",
    "        self.conv.add_module(\"relu_2\", nn.ReLU())\n",
    "\n",
    "        self.dense = nn.Sequential()\n",
    "        self.dense.add_module('flatten', nn.Flatten())\n",
    "        self.dense.add_module('linear', nn.Linear(6144, 1))\n",
    "        self.dense.add_module('sigmoid', nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose(1, 2)\n",
    "        x = self.conv(x)\n",
    "        y = self.dense(x)\n",
    "        return y\n",
    "\n",
    "net = Net()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(10000, 3, padding_idx=1) \n",
      "\n",
      "Sequential(\n",
      "  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))\n",
      "  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_1): ReLU()\n",
      "  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))\n",
      "  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_2): ReLU()\n",
      ") \n",
      "\n",
      "Sequential(\n",
      "  (flatten): Flatten()\n",
      "  (linear): Linear(in_features=6144, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ") \n",
      "\n",
      "child number 3\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for child in net.children():\n",
    "    i += 1\n",
    "    print(child, \"\\n\")\n",
    "\n",
    "print(\"child number\", i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding : Embedding(10000, 3, padding_idx=1) \n",
      "\n",
      "conv : Sequential(\n",
      "  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))\n",
      "  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_1): ReLU()\n",
      "  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))\n",
      "  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_2): ReLU()\n",
      ") \n",
      "\n",
      "dense : Sequential(\n",
      "  (flatten): Flatten()\n",
      "  (linear): Linear(in_features=6144, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ") \n",
      "\n",
      "child number 3\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for name, child in net.named_children():\n",
    "    i += 1\n",
    "    print(name, \":\", child, \"\\n\")\n",
    "print(\"child number\", i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (embedding): Embedding(10000, 3, padding_idx=1)\n",
      "  (conv): Sequential(\n",
      "    (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))\n",
      "    (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (relu_1): ReLU()\n",
      "    (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))\n",
      "    (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (relu_2): ReLU()\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (flatten): Flatten()\n",
      "    (linear): Linear(in_features=6144, out_features=1, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Embedding(10000, 3, padding_idx=1)\n",
      "Sequential(\n",
      "  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))\n",
      "  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_1): ReLU()\n",
      "  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))\n",
      "  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_2): ReLU()\n",
      ")\n",
      "Conv1d(3, 16, kernel_size=(5,), stride=(1,))\n",
      "MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "ReLU()\n",
      "Conv1d(16, 128, kernel_size=(2,), stride=(1,))\n",
      "MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "ReLU()\n",
      "Sequential(\n",
      "  (flatten): Flatten()\n",
      "  (linear): Linear(in_features=6144, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Flatten()\n",
      "Linear(in_features=6144, out_features=1, bias=True)\n",
      "Sigmoid()\n",
      "module number: 13\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for module in net.modules():\n",
    "    i += 1\n",
    "    print(module)\n",
    "print(\"module number:\", i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding': Embedding(10000, 3, padding_idx=1), 'conv': Sequential(\n",
      "  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))\n",
      "  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_1): ReLU()\n",
      "  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))\n",
      "  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu_2): ReLU()\n",
      "), 'dense': Sequential(\n",
      "  (flatten): Flatten()\n",
      "  (linear): Linear(in_features=6144, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")}\n"
     ]
    },
    {
     "data": {
      "text/plain": "Embedding(10000, 3, padding_idx=1)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "children_dict = {name:module for name,module in net.named_children()}\n",
    "\n",
    "print(children_dict)\n",
    "embedding = children_dict[\"embedding\"]\n",
    "embedding.requires_grad_(False) #冻结其参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "#可以看到其第一层的参数已经不可以被训练了。\n",
    "for param in embedding.parameters():\n",
    "    print(param.requires_grad)\n",
    "    print(param.numel())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1               [-1, 200, 3]          30,000\n",
      "            Conv1d-2              [-1, 16, 196]             256\n",
      "         MaxPool1d-3               [-1, 16, 98]               0\n",
      "              ReLU-4               [-1, 16, 98]               0\n",
      "            Conv1d-5              [-1, 128, 97]           4,224\n",
      "         MaxPool1d-6              [-1, 128, 48]               0\n",
      "              ReLU-7              [-1, 128, 48]               0\n",
      "           Flatten-8                 [-1, 6144]               0\n",
      "            Linear-9                    [-1, 1]           6,145\n",
      "          Sigmoid-10                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 40,625\n",
      "Trainable params: 10,625\n",
      "Non-trainable params: 30,000\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.000763\n",
      "Forward/backward pass size (MB): 0.287796\n",
      "Params size (MB): 0.154972\n",
      "Estimated Total Size (MB): 0.443531\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchkeras import summary\n",
    "summary(net,input_shape=(200,), input_dtype=torch.LongTensor)\n",
    "# 不可训练参数数量增加\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}