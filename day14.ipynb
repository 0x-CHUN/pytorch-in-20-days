{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dataset和DataLoader\n",
    "\n",
    "Pytorch通常使用Dataset和DataLoader这两个工具类来构建数据管道。\n",
    "\n",
    "Dataset定义了数据集的内容，它相当于一个类似列表的数据结构，具有确定的长度，能够用索引获取数据集中的元素。\n",
    "\n",
    "而DataLoader定义了按batch加载数据集的方法，它是一个实现了__iter__方法的可迭代对象，每次迭代输出一个batch的数据。\n",
    "\n",
    "DataLoader能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。\n",
    "\n",
    "在绝大部分情况下，用户只需实现Dataset的__len__方法和__getitem__方法，就可以轻松构建自己的数据集，并用默认数据管道进行加载。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset和DataLoader概述\n",
    "\n",
    "获取一个batch的数据的步骤：\n",
    "\n",
    "1. 确定数据集的长度\n",
    "2. 抽取batch_size个数\n",
    "3. 从数据集中去对应下标的元素\n",
    "4. 整理成Tensor输出"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataset和 DataLoader的核心接口逻辑伪代码\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self,dataset,batch_size,collate_fn,shuffle = True,drop_last = False):\n",
    "        self.dataset = dataset\n",
    "        self.sampler =torch.utils.data.RandomSampler if shuffle else \\\n",
    "           torch.utils.data.SequentialSampler\n",
    "        self.batch_sampler = torch.utils.data.BatchSampler\n",
    "        self.sample_iter = self.batch_sampler(\n",
    "            self.sampler(range(len(dataset))),\n",
    "            batch_size = batch_size,drop_last = drop_last)\n",
    "\n",
    "    def __next__(self):\n",
    "        indices = next(self.sample_iter)\n",
    "        batch = self.collate_fn([self.dataset[i] for i in indices])\n",
    "        return batch\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 使用Dataset创建数据集\n",
    "\n",
    "Dataset创建数据集常用的方法有：\n",
    "\n",
    "* 使用 torch.utils.data.TensorDataset 根据Tensor创建数据集(numpy的array，Pandas的DataFrame需要先转换成Tensor)。\n",
    "* 使用 torchvision.datasets.ImageFolder 根据图片目录创建图片数据集。\n",
    "* 继承 torch.utils.data.Dataset 创建自定义数据集。\n",
    "\n",
    "此外，还可以通过\n",
    "\n",
    "torch.utils.data.random_split 将一个数据集分割成多份，常用于分割训练集，验证集和测试集。\n",
    "\n",
    "调用Dataset的加法运算符(+)将多个数据集合并成一个数据集。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 根据Tensor创建数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataset.TensorDataset'>\n",
      "<class 'torch.utils.data.dataset.Subset'>\n",
      "150\n",
      "120\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "ds_iris = TensorDataset(torch.tensor(iris.data), torch.tensor(iris.target))\n",
    "\n",
    "n_train = int(len(ds_iris) * 0.8)\n",
    "n_valid = len(ds_iris) - n_train\n",
    "ds_train, ds_valid = random_split(ds_iris, [n_train, n_valid])\n",
    "\n",
    "print(type(ds_iris))\n",
    "print(type(ds_train))\n",
    "print(len(ds_iris))\n",
    "print(len(ds_train))\n",
    "print(len(ds_valid))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.5000, 4.2000, 1.4000, 0.2000],\n",
      "        [5.1000, 3.7000, 1.5000, 0.4000],\n",
      "        [6.5000, 3.0000, 5.8000, 2.2000],\n",
      "        [6.4000, 2.8000, 5.6000, 2.1000],\n",
      "        [5.7000, 3.0000, 4.2000, 1.2000],\n",
      "        [5.8000, 2.7000, 4.1000, 1.0000],\n",
      "        [4.6000, 3.4000, 1.4000, 0.3000],\n",
      "        [4.4000, 3.2000, 1.3000, 0.2000]], dtype=torch.float64)\n",
      "tensor([0, 0, 2, 2, 1, 1, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "dl_train, dl_valid = DataLoader(ds_train, batch_size=8), DataLoader(ds_valid, batch_size=8)\n",
    "for features, labels in dl_valid:\n",
    "    print(features)\n",
    "    print(labels)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(ds_train) =  120\n",
      "len(ds_valid) =  30\n",
      "len(ds_train+ds_valid) =  150\n",
      "<class 'torch.utils.data.dataset.ConcatDataset'>\n"
     ]
    }
   ],
   "source": [
    "ds_data = ds_train + ds_valid\n",
    "\n",
    "print('len(ds_train) = ', len(ds_train))\n",
    "print('len(ds_valid) = ', len(ds_valid))\n",
    "print('len(ds_train+ds_valid) = ', len(ds_data))\n",
    "\n",
    "print(type(ds_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 根据图片目录创建图片数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 定义图片增强操作\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0_airplane': 0, '1_automobile': 1}\n"
     ]
    }
   ],
   "source": [
    "ds_train = datasets.ImageFolder(\"./data/cifar2/train/\",\n",
    "                                transform=transform_train,\n",
    "                                target_transform=lambda t:torch.tensor([t]).float())\n",
    "ds_valid = datasets.ImageFolder(\"./data/cifar2/test/\",\n",
    "                                transform=transform_val,\n",
    "                                target_transform=lambda t:torch.tensor([t]).float())\n",
    "\n",
    "print(ds_train.class_to_idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=50, shuffle=True)\n",
    "dl_valid = DataLoader(ds_valid, batch_size=50, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 3, 32, 32])\n",
      "torch.Size([50, 1])\n"
     ]
    }
   ],
   "source": [
    "for features, labels in dl_train:\n",
    "    print(features.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import string\n",
    "\n",
    "MAX_WORDS = 10000 # 仅考虑最高频的10000个词\n",
    "MAX_LEN = 200 # 每个样本保留200个词的长度\n",
    "BATCH_SIZE = 20\n",
    "train_data_path = './data/imdb/train.tsv'\n",
    "test_data_path = './data/imdb/test.tsv'\n",
    "train_token_path = './data/imdb/train_token.tsv'\n",
    "test_token_path = './data/imdb/test_token.tsv'\n",
    "train_samples_path = './data/imdb/train_samples/'\n",
    "test_samples_path = './data/imdb/test_samples/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "      count  word_id\nthe  268230        2\nand  129713        3\na    129479        4\nof   116497        5\nto   108296        6\nis    85615        7\n      84074        8\nin    74715        9\nit    62587       10\ni     60837       11",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>word_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>the</th>\n      <td>268230</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>129713</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>a</th>\n      <td>129479</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>of</th>\n      <td>116497</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>to</th>\n      <td>108296</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>is</th>\n      <td>85615</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th></th>\n      <td>84074</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>in</th>\n      <td>74715</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>it</th>\n      <td>62587</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>i</th>\n      <td>60837</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建词典\n",
    "\n",
    "word_count_dict = {}\n",
    "\n",
    "# 清洗文本\n",
    "\n",
    "def clean_text(text):\n",
    "    lowercase = text.lower().replace(\"\\n\", \" \")\n",
    "    stripped_html = re.sub('<br />', ' ', lowercase)\n",
    "    cleaned_punctuation = re.sub('[%s]'%re.escape(string.punctuation), '', stripped_html)\n",
    "    return cleaned_punctuation\n",
    "\n",
    "with open(train_data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        label, text = line.split('\\t')\n",
    "        cleaned_text = clean_text(text)\n",
    "        for word in cleaned_text.split(' '):\n",
    "            word_count_dict[word] = word_count_dict.get(word, 0) + 1\n",
    "\n",
    "df_word_dict = pd.DataFrame(pd.Series(word_count_dict, name='count'))\n",
    "df_word_dict = df_word_dict.sort_values(by='count', ascending=False)\n",
    "\n",
    "df_word_dict = df_word_dict[0 : MAX_WORDS-2]\n",
    "df_word_dict['word_id'] = range(2, MAX_WORDS)\n",
    "\n",
    "word_id_dict = df_word_dict['word_id'].to_dict()\n",
    "\n",
    "df_word_dict.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# 转换token\n",
    "\n",
    "# 填充文本\n",
    "\n",
    "def pad(data_list, pad_length):\n",
    "    padded_list = data_list.copy()\n",
    "    if len(data_list) > pad_length:\n",
    "        padded_list = data_list[-pad_length:]\n",
    "    if len(data_list) < pad_length:\n",
    "        padded_list = [1] * (pad_length - len(data_list)) + data_list\n",
    "    return padded_list\n",
    "\n",
    "def text_to_token(text_file, token_file):\n",
    "    with open(text_file, 'r', encoding='utf-8') as fin,\\\n",
    "        open(token_file, 'w', encoding='utf-8') as fout:\n",
    "        for line in fin:\n",
    "            label, text = line.split('\\t')\n",
    "            cleaned_text = clean_text(text)\n",
    "            word_token_list = [word_id_dict.get(word, 0) for word in cleaned_text.split(' ')]\n",
    "            pad_list = pad(word_token_list, MAX_LEN)\n",
    "            out_line = label + '\\t' + \" \".join(str(x) for x in pad_list)\n",
    "            fout.write(out_line + '\\n')\n",
    "\n",
    "text_to_token(train_data_path, train_token_path)\n",
    "text_to_token(test_data_path, test_token_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(train_samples_path):\n",
    "    os.mkdir(train_samples_path)\n",
    "\n",
    "if not os.path.exists(test_samples_path):\n",
    "    os.mkdir(test_samples_path)\n",
    "\n",
    "def split_samples(token_path, samples_dir):\n",
    "    with open(token_path, 'r', encoding='utf-8') as fin:\n",
    "        i = 0\n",
    "        for line in fin:\n",
    "            with open(samples_dir+\"%d.txt\"%i, 'w', encoding='utf-8') as fout:\n",
    "                fout.write(line)\n",
    "            i = i + 1\n",
    "\n",
    "split_samples(train_token_path, train_samples_path)\n",
    "split_samples(test_token_path, test_samples_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.txt', '1.txt', '10.txt', '100.txt', '1000.txt', '10000.txt', '10001.txt', '10002.txt', '10003.txt', '10004.txt']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(train_samples_path)[0:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "5000\n",
      "tensor([[   1,    1,    1,  ...,   17,    0,    8],\n",
      "        [   1,    1,    1,  ..., 7917, 3554,    8],\n",
      "        [  61,  610,   21,  ...,  162,   10,    8],\n",
      "        ...,\n",
      "        [   1,    1,    1,  ...,   34, 2166,    8],\n",
      "        [   1,    1,    1,  ...,  114,  460,    8],\n",
      "        [   1,    1,    1,  ..., 5112,  218,    8]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from  torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class imdbDataset(Dataset):\n",
    "    def __init__(self, samples_dir):\n",
    "        self.samples_dir = samples_dir\n",
    "        self.samples_paths = os.listdir(samples_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.samples_dir + self.samples_paths[index]\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            line = f.readline()\n",
    "            label, tokens = line.split('\\t')\n",
    "            label = torch.tensor([float(label)], dtype=torch.float)\n",
    "            feature = torch.tensor([int(x) for x in tokens.split(' ')], dtype=torch.long)\n",
    "            return (feature, label)\n",
    "\n",
    "ds_train = imdbDataset(train_samples_path)\n",
    "ds_test = imdbDataset(test_samples_path)\n",
    "\n",
    "print(len(ds_train))\n",
    "print(len(ds_test))\n",
    "\n",
    "dl_train = DataLoader(dataset=ds_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dl_test = DataLoader(dataset=ds_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "for feature, label in dl_train:\n",
    "    print(feature)\n",
    "    print(label)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 使用DataLoader加载数据集\n",
    "\n",
    "DataLoader能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。\n",
    "\n",
    "DataLoader的函数签名如下:\n",
    "\n",
    "```\n",
    "DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    sampler=None,\n",
    "    batch_sampler=None,\n",
    "    num_workers=0,\n",
    "    collate_fn=None,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    timeout=0,\n",
    "    worker_init_fn=None,\n",
    "    multiprocessing_context=None,\n",
    ")\n",
    "```\n",
    "\n",
    "一般情况下，我们仅仅会配置 dataset, batch_size, shuffle, num_workers, drop_last这五个参数，其他参数使用默认值即可。\n",
    "\n",
    "DataLoader除了可以加载torch.utils.data.Dataset外，还能够加载另外一种数据集 torch.utils.data.IterableDataset。\n",
    "\n",
    "和Dataset数据集相当于一种列表结构不同，IterableDataset相当于一种迭代器结构。 它更加复杂，一般较少使用。\n",
    "\n",
    "* dataset : 数据集\n",
    "* batch_size: 批次大小\n",
    "* shuffle: 是否乱序\n",
    "* sampler: 样本采样函数，一般无需设置。\n",
    "* batch_sampler: 批次采样函数，一般无需设置。\n",
    "* num_workers: 使用多进程读取数据，设置的进程数。\n",
    "* collate_fn: 整理一个批次数据的函数。\n",
    "* pin_memory: 是否设置为锁业内存。默认为False，锁业内存不会使用虚拟内存(硬盘)，从锁业内存拷贝到GPU上速度会更快。\n",
    "* drop_last: 是否丢弃最后一个样本数量不足batch_size批次数据。\n",
    "* timeout: 加载一个数据批次的最长等待时间，一般无需设置。\n",
    "* worker_init_fn: 每个worker中dataset的初始化函数，常用于 IterableDataset。一般不使用。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([41, 43,  1, 28,  5, 45, 18, 22, 13, 49])\n",
      "tensor([10,  4, 34, 26, 21, 38, 32, 25,  7, 44])\n",
      "tensor([19, 23,  3, 27, 17,  2,  6,  9, 36, 24])\n",
      "tensor([39,  8, 37, 31, 42, 48, 35, 33, 12, 29])\n"
     ]
    }
   ],
   "source": [
    "#构建输入数据管道\n",
    "ds = TensorDataset(torch.arange(1,50))\n",
    "dl = DataLoader(ds,\n",
    "                batch_size = 10,\n",
    "                shuffle= True,\n",
    "                num_workers=2,\n",
    "                drop_last = True)\n",
    "#迭代数据\n",
    "for batch, in dl:\n",
    "    print(batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}